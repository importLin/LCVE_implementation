{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = \"UCF101_subset\"\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "test_video_file_paths = (list(dataset_root_path.glob(\"test/*/*.avi\")))\n",
    "class_labels = sorted({path.parent.name for path in test_video_file_paths})\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T03:00:28.781485Z",
     "start_time": "2024-06-25T03:00:28.778083Z"
    }
   },
   "id": "54a1ac9f1baad5ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "import pytorchvideo.data\n",
    "import os\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "resize_to = (224, 224)\n",
    "num_frames_to_sample = 32\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to, antialias=False),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T03:00:29.651978Z",
     "start_time": "2024-06-25T03:00:28.922574Z"
    }
   },
   "id": "37533a857374ae4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/vivit/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T03:00:29.655006Z",
     "start_time": "2024-06-25T03:00:29.652584Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference(model, batch, device):\n",
    "    inputs = {\n",
    "        # bs, 3, 32, 224, 224 - > bs, 32, 3, 224, 224\n",
    "        \"pixel_values\": batch['video'].transpose(1, 2)\n",
    "    }\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return predictions"
   ],
   "id": "15b7938ece892e9b",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd119a4b3b05cedd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T03:00:31.008877Z",
     "start_time": "2024-06-25T03:00:29.655560Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import VivitForVideoClassification\n",
    "from model_encryption import weight_extracting, weight_reloading\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "key_dict = np.load(\"key_dicts/key-32-2-16-seed100.npy\", allow_pickle=True).item()\n",
    "model_ckpt = \"checkpoints/vivit-b-16x2-kinetics400-finetuned-ucf101-subsetâ€”withouImgP/checkpoint-370\"\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=False,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "testing_loader = DataLoader(test_dataset, batch_size=5)"
   ],
   "id": "440bb771471fc352",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classification with plain videos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37a9dc25bba10a"
  },
  {
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "for i, batch in enumerate(testing_loader):\n",
    "    predictions = inference(model, batch, device)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "acc = metric.compute()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T03:00:46.449540Z",
     "start_time": "2024-06-25T03:00:31.009721Z"
    }
   },
   "id": "a8bfa0bf8de71173",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classification with encrypted videos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa89468c2542648d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T03:00:46.454224Z",
     "start_time": "2024-06-25T03:00:46.450113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_encryption import cube_embedding_shuffling, pos_embedding_shuffling\n",
    "\n",
    "# model encryption \n",
    "ce_weight, pos_weight = weight_extracting(model.vivit.embeddings)\n",
    "shuffled_ce_weight = cube_embedding_shuffling(ce_weight, key_dict['ce_key'])\n",
    "shuffled_pos_weight = pos_embedding_shuffling(pos_weight, key_dict['pos_key'])\n",
    "\n",
    "# reload weights\n",
    "model.vivit.embeddings = weight_reloading(model.vivit.embeddings, shuffled_ce_weight, pos_weight)"
   ],
   "id": "ecd8ec532e04ec8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling weight of Patch embedding...\n",
      "Shuffling weight of Position embedding...\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T03:01:01.030004Z",
     "start_time": "2024-06-25T03:00:46.454686Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from video_encryption import *\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "for i, batch in enumerate(testing_loader):\n",
    "    video_tensor = batch['video'].transpose(1, 2)\n",
    "    \n",
    "    # encryption \n",
    "    cube_group = cube_division(video_tensor)\n",
    "    cube_group = cube_pix_shuffling(cube_group, key_dict['ce_key'])\n",
    "    cube_group = cube_pos_shuffling(cube_group, key_dict['pos_key'])\n",
    "    encrypted_video = cube_integration(cube_group).transpose(1, 2)\n",
    "    batch['video'] = encrypted_video\n",
    "\n",
    "    predictions = inference(model, batch, device)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "\n",
    "acc = metric.compute()"
   ],
   "id": "131db118d7c8e2fa",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T03:01:01.032088Z",
     "start_time": "2024-06-25T03:01:01.030659Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "3418fb04f5dda3fd",
   "outputs": [],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
