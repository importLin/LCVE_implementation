{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = \"UCF101_subset\"\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "test_video_file_paths = (list(dataset_root_path.glob(\"test/*/*.avi\")))\n",
    "class_labels = sorted({path.parent.name for path in test_video_file_paths})\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "print(f\"Unique classes: {list(label2id.keys())}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T02:14:52.637173Z",
     "start_time": "2024-06-25T02:14:52.633408Z"
    }
   },
   "id": "54a1ac9f1baad5ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress'].\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import pytorchvideo.data\n",
    "import os\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    Resize,\n",
    ")\n",
    "\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "resize_to = (224, 224)\n",
    "num_frames_to_sample = 32\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    UniformTemporalSubsample(num_frames_to_sample),\n",
    "                    Lambda(lambda x: x / 255.0),\n",
    "                    Normalize(mean, std),\n",
    "                    Resize(resize_to, antialias=False),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T02:14:56.425137Z",
     "start_time": "2024-06-25T02:14:55.737195Z"
    }
   },
   "id": "37533a857374ae4b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/anaconda3/envs/vivit/lib/python3.11/site-packages/torchvision/transforms/functional_tensor.py:5: UserWarning: The torchvision.transforms.functional_tensor module is deprecated in 0.15 and will be **removed in 0.17**. Please don't rely on it. You probably just need to use APIs in torchvision.transforms.functional or in torchvision.transforms.v2.functional.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T02:14:56.428349Z",
     "start_time": "2024-06-25T02:14:56.425848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def inference(model, batch, device):\n",
    "    inputs = {\n",
    "        # bs, 3, 32, 224, 224 - > bs, 32, 3, 224, 224\n",
    "        \"pixel_values\": batch['video'].transpose(1, 2)\n",
    "    }\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    return predictions"
   ],
   "id": "15b7938ece892e9b",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "Initialization"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cd119a4b3b05cedd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T02:14:58.041460Z",
     "start_time": "2024-06-25T02:14:56.739307Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import VivitForVideoClassification\n",
    "from model_encryption import weight_extracting, weight_reloading\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "key_dict = np.load(\"key_dicts/key-32-2-16-seed100.npy\", allow_pickle=True).item()\n",
    "model_ckpt = \"checkpoints/vivit-b-16x2-kinetics400-finetuned-ucf101-subset—withouImgP/checkpoint-370\"\n",
    "model = VivitForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=False,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    ")\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "testing_loader = DataLoader(test_dataset, batch_size=5)"
   ],
   "id": "440bb771471fc352",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classification with plain videos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b37a9dc25bba10a"
  },
  {
   "cell_type": "code",
   "source": [
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "for i, batch in enumerate(testing_loader):\n",
    "    predictions = inference(model, batch, device)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "acc = metric.compute()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-25T02:15:13.544861Z",
     "start_time": "2024-06-25T02:14:58.042251Z"
    }
   },
   "id": "a8bfa0bf8de71173",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9655172413793104}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classification with encrypted videos"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aa89468c2542648d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T02:15:13.548854Z",
     "start_time": "2024-06-25T02:15:13.545396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from model_encryption import cube_embedding_shuffling, pos_embedding_shuffling\n",
    "\n",
    "# model encryption \n",
    "ce_weight, pos_weight = weight_extracting(model.vivit.embeddings)\n",
    "shuffled_ce_weight = cube_embedding_shuffling(ce_weight, key_dict['ce_key'])\n",
    "shuffled_pos_weight = pos_embedding_shuffling(pos_weight, key_dict['pos_key'])\n",
    "\n",
    "# reload weights\n",
    "model.vivit.embeddings = weight_reloading(model.vivit.embeddings, shuffled_ce_weight, pos_weight)"
   ],
   "id": "ecd8ec532e04ec8b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffling weight of Patch embedding...\n",
      "Shuffling weight of Position embedding...\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T02:15:29.559666Z",
     "start_time": "2024-06-25T02:15:13.549453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from video_encryption import *\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "for i, batch in enumerate(testing_loader):\n",
    "    video_tensor = batch['video'].transpose(1, 2)\n",
    "    \n",
    "    # encryption \n",
    "    cube_group = cube_division(video_tensor)\n",
    "    cube_group = cube_pix_shuffling(cube_group, key_dict['ce_key'])\n",
    "    cube_group = cube_pos_shuffling(cube_group, key_dict['pos_key'])\n",
    "    encrypted_video = cube_integration(cube_group).transpose(1, 2)\n",
    "    batch['video'] = encrypted_video\n",
    "\n",
    "    predictions = inference(model, batch, device)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"label\"])\n",
    "\n",
    "acc = metric.compute()"
   ],
   "id": "131db118d7c8e2fa",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.9655172413793104}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T02:38:16.726043Z",
     "start_time": "2024-06-25T02:38:16.721543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "# 创建一个 4x4 的张量\n",
    "input_tensor = torch.randn(1, 3, 224, 224).float()\n",
    "\n",
    "print(\"输入张量:\")\n",
    "print(input_tensor)\n",
    "\n",
    "# 使用 unfold 展开 2x2 的块\n",
    "unfolded = torch.nn.functional.unfold(input_tensor, kernel_size=16, stride=16)\n",
    "\n",
    "print(\"\\nunfold 后的张量:\")\n",
    "print(unfolded)\n"
   ],
   "id": "3418fb04f5dda3fd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入张量:\n",
      "tensor([[[[ 0.0602,  1.7181,  1.3446,  ..., -1.1445,  0.9502, -0.7615],\n",
      "          [-1.8114,  0.1623, -2.0558,  ..., -0.5184, -2.4697, -0.0365],\n",
      "          [ 0.7450, -0.2364, -0.7323,  ...,  0.3852, -0.3082,  0.7032],\n",
      "          ...,\n",
      "          [ 2.1919,  0.2222, -0.7634,  ...,  0.5040,  0.3767, -0.6772],\n",
      "          [-0.5092,  1.5969, -0.2061,  ..., -1.2855, -0.1036,  0.2316],\n",
      "          [-1.2236,  0.9498, -0.3940,  ..., -0.7307,  0.1755, -1.3499]],\n",
      "\n",
      "         [[-0.0567, -0.2995, -0.1699,  ...,  0.4523,  0.3211,  1.5771],\n",
      "          [-0.5349,  0.9403, -1.4165,  ...,  0.1697,  2.0208,  0.0340],\n",
      "          [ 1.2738,  0.6613,  0.8278,  ..., -0.7007,  0.1385,  0.3181],\n",
      "          ...,\n",
      "          [-1.7200, -0.0846,  0.7557,  ...,  0.2991, -0.1352, -2.1994],\n",
      "          [ 0.4339,  0.8423,  0.8979,  ..., -0.0877,  0.0953,  2.6410],\n",
      "          [-0.7873,  0.9222, -1.2158,  ...,  0.4778, -1.4454, -1.1518]],\n",
      "\n",
      "         [[ 0.9152,  1.8353,  0.4381,  ..., -1.6033, -0.8731, -0.2577],\n",
      "          [ 2.1494,  0.1231,  3.2908,  ..., -0.5093,  2.1206,  0.4150],\n",
      "          [-1.4475,  1.1716,  0.8219,  ..., -0.8045,  0.1050,  1.4239],\n",
      "          ...,\n",
      "          [ 1.3728,  1.3081, -1.2034,  ..., -0.4807, -1.0052, -0.5773],\n",
      "          [ 1.0569, -1.0289, -0.3094,  ..., -0.9724, -0.2322,  0.4625],\n",
      "          [-0.1684,  0.1767, -0.5102,  ..., -0.0956, -0.7356,  1.2399]]]])\n",
      "\n",
      "unfold 后的张量:\n",
      "tensor([[[ 0.0602,  0.6254,  1.1399,  ...,  1.0153, -0.3031, -0.6993],\n",
      "         [ 1.7181,  0.9192, -1.4680,  ..., -1.3723, -1.0965, -0.8489],\n",
      "         [ 1.3446, -1.1684,  0.4630,  ..., -2.6756,  0.9997, -1.9105],\n",
      "         ...,\n",
      "         [-0.9198,  0.1960, -0.3541,  ..., -0.6296, -1.2180, -0.0956],\n",
      "         [-0.8663,  1.3804,  1.0156,  ..., -0.5488,  1.7293, -0.7356],\n",
      "         [-0.3611,  0.0061, -0.9260,  ...,  1.9497,  1.1686,  1.2399]]])\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-25T02:38:21.127704Z",
     "start_time": "2024-06-25T02:38:21.124729Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "54f95b991fa2cc70",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0602,  0.6254,  1.1399,  ...,  1.0153, -0.3031, -0.6993],\n",
      "         [ 1.7181,  0.9192, -1.4680,  ..., -1.3723, -1.0965, -0.8489],\n",
      "         [ 1.3446, -1.1684,  0.4630,  ..., -2.6756,  0.9997, -1.9105],\n",
      "         ...,\n",
      "         [-0.9198,  0.1960, -0.3541,  ..., -0.6296, -1.2180, -0.0956],\n",
      "         [-0.8663,  1.3804,  1.0156,  ..., -0.5488,  1.7293, -0.7356],\n",
      "         [-0.3611,  0.0061, -0.9260,  ...,  1.9497,  1.1686,  1.2399]]])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6ff38dba30e7e505"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
